{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Homework\n",
    "\n",
    "In this homework we'll put what we learned about Spark\n",
    "in practice.\n",
    "\n",
    "We'll use high volume for-hire vehicles (HVFHV) dataset for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. Install Spark and PySpark\n",
    "\n",
    "* Install Spark\n",
    "* Run PySpark\n",
    "* Create a local spark session \n",
    "* Execute `spark.version`\n",
    "\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 answer (correct)\n",
    "\n",
    "```\n",
    "3.0.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. HVFHW February 2021\n",
    "\n",
    "Download the HVFHV data for february 2021:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-28 18:28:19--  https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
      "Resolving nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)... 52.217.90.236\n",
      "Connecting to nyc-tlc.s3.amazonaws.com (nyc-tlc.s3.amazonaws.com)|52.217.90.236|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 733822658 (700M) [text/csv]\n",
      "Saving to: ‘fhvhv_tripdata_2021-02.csv.1’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 699.83M  36.5MB/s    in 20s     \n",
      "\n",
      "2022-02-28 18:28:39 (35.3 MB/s) - ‘fhvhv_tripdata_2021-02.csv.1’ saved [733822658/733822658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read it with Spark using the same schema as we did \n",
    "in the lessons. We will use this dataset for all\n",
    "the remaining questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "        types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "        types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "        types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "        types.StructField('PULocationID', types.IntegerType(), True),\n",
    "        types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "        types.StructField('SR_Flag', types.StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:26:02|2021-02-01 00:42:51|         208|         243|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:45:50|2021-02-01 01:02:50|         243|         220|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:06:42|2021-02-01 00:31:50|          49|          37|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:34:34|2021-02-01 00:58:13|          37|          76|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:03:43|2021-02-01 00:39:37|          80|         241|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:55:36|2021-02-01 01:08:39|         174|          51|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:06:13|2021-02-01 00:33:45|         235|         129|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:42:24|2021-02-01 01:11:31|         129|         169|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:07:05|2021-02-01 00:20:53|         226|          82|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:28:56|2021-02-01 00:33:59|          82|         129|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:44:53|2021-02-01 01:07:54|           7|          79|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:17:55|2021-02-01 00:34:41|           4|         170|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:38:14|2021-02-01 00:59:20|         164|          42|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:08:04|2021-02-01 00:24:41|         237|           4|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:30:44|2021-02-01 00:41:26|         107|          45|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition it to 24 partitions and save it to\n",
    "parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.repartition(24).write.parquet('fhvhv/2021/02/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the size of the folder with results (in MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216M\tfhvhv/2021/01\n",
      "210M\tfhvhv/2021/02\n",
      "426M\tfhvhv/2021\n",
      "426M\tfhvhv\n"
     ]
    }
   ],
   "source": [
    "!du -h fhvhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposed command\n",
    "!ls -lh fhvhv/2021/02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh fhvhv_tripdata_2021-02.csv\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')\n",
    "\n",
    "df = df.repartition(24)\n",
    "\n",
    "df.write.parquet('data/pq/fhvhv/2021/02/', compression=)\n",
    "\n",
    "df = spark.read.parquet('data/pq/fhvhv/2021/02/')\n",
    "\n",
    "!ls -lh data/pq/fhvhv/2021/02/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 answer (correct)\n",
    "\n",
    "```\n",
    "210MB\n",
    "\n",
    "In the proposed answers, the closest one is 208MB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Count records \n",
    "\n",
    "How many taxi trips were there on February 15?\n",
    "\n",
    "Consider only trips that started on February 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('fhvhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  367170|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    fhvhv\n",
    "WHERE\n",
    "    DATE(pickup_datetime) = '2021-02-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2021-02-15'\") \\\n",
    "    .count()\n",
    "\n",
    "df.registerTempTable('fhvhv_2021_02')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhvhv_2021_02\n",
    "WHERE\n",
    "    to_date(pickup_datetime) = '2021-02-15';\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 answer (correct)\n",
    "\n",
    "```\n",
    "367170\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc,desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+----------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|duration_seconds|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+----------------+\n",
      "|           HV0005|              B02510|2021-02-11 13:40:44|2021-02-12 10:39:44|         247|          41|   null|           75540|\n",
      "|           HV0004|              B02800|2021-02-17 15:54:53|2021-02-18 07:48:34|         242|         254|   null|           57221|\n",
      "|           HV0004|              B02800|2021-02-20 12:08:15|2021-02-21 00:22:14|         188|          55|   null|           44039|\n",
      "|           HV0003|              B02864|2021-02-03 20:24:25|2021-02-04 07:41:58|          51|         147|   null|           40653|\n",
      "|           HV0003|              B02887|2021-02-19 23:17:44|2021-02-20 09:44:01|         210|         149|   null|           37577|\n",
      "|           HV0003|              B02764|2021-02-25 17:13:35|2021-02-26 02:57:05|         174|         126|   null|           35010|\n",
      "|           HV0003|              B02875|2021-02-20 01:36:13|2021-02-20 11:16:19|         242|          31|   null|           34806|\n",
      "|           HV0005|              B02510|2021-02-18 15:24:19|2021-02-19 01:01:11|         196|         197|   null|           34612|\n",
      "|           HV0003|              B02764|2021-02-18 01:31:20|2021-02-18 11:07:15|          89|         265|   null|           34555|\n",
      "|           HV0005|              B02510|2021-02-10 20:51:39|2021-02-11 06:21:08|         254|         259|   null|           34169|\n",
      "|           HV0003|              B02764|2021-02-10 01:56:17|2021-02-10 10:57:33|          61|         265|   null|           32476|\n",
      "|           HV0005|              B02510|2021-02-25 09:18:18|2021-02-25 18:18:57|         169|         265|   null|           32439|\n",
      "|           HV0005|              B02510|2021-02-21 19:59:13|2021-02-22 04:56:16|          10|          10|   null|           32223|\n",
      "|           HV0003|              B02864|2021-02-09 18:36:13|2021-02-10 03:31:00|          78|         147|   null|           32087|\n",
      "|           HV0004|              B02800|2021-02-06 09:48:09|2021-02-06 18:32:16|         229|         188|   null|           31447|\n",
      "|           HV0005|              B02510|2021-02-02 09:42:30|2021-02-02 18:17:43|          85|          85|   null|           30913|\n",
      "|           HV0005|              B02510|2021-02-10 10:12:08|2021-02-10 18:46:24|          29|         125|   null|           30856|\n",
      "|           HV0003|              B02764|2021-02-09 13:30:13|2021-02-09 22:02:25|         188|         265|   null|           30732|\n",
      "|           HV0005|              B02510|2021-02-21 22:50:52|2021-02-22 07:21:52|         177|          73|   null|           30660|\n",
      "|           HV0005|              B02510|2021-02-05 21:32:33|2021-02-06 06:01:04|          97|          72|   null|           30511|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration_seconds', df.dropoff_datetime.cast('long')-df.pickup_datetime.cast('long')) \\\n",
    "    .orderBy(col('duration_seconds').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+\n",
      "|    pickup_datetime|   dropoff_datetime|duration|\n",
      "+-------------------+-------------------+--------+\n",
      "|2021-02-11 13:40:44|2021-02-12 10:39:44|   75540|\n",
      "|2021-02-17 15:54:53|2021-02-18 07:48:34|   57221|\n",
      "|2021-02-25 09:18:18|2021-02-25 18:18:57|   32439|\n",
      "|2021-02-12 06:16:42|2021-02-12 14:39:10|   30148|\n",
      "|2021-02-10 15:00:54|2021-02-10 22:49:57|   28143|\n",
      "|2021-02-09 12:40:43|2021-02-09 20:04:03|   26600|\n",
      "|2021-02-23 08:02:37|2021-02-23 14:49:56|   24439|\n",
      "|2021-02-24 10:54:08|2021-02-24 17:28:37|   23669|\n",
      "|2021-02-05 14:09:56|2021-02-05 20:30:46|   22850|\n",
      "|2021-02-11 12:36:26|2021-02-11 18:50:42|   22456|\n",
      "|2021-02-17 10:34:15|2021-02-17 16:39:59|   21944|\n",
      "|2021-02-26 16:34:50|2021-02-26 22:22:23|   20853|\n",
      "|2021-02-24 13:20:32|2021-02-24 19:01:48|   20476|\n",
      "|2021-02-10 15:23:08|2021-02-10 21:01:32|   20304|\n",
      "|2021-02-08 18:26:37|2021-02-08 23:43:52|   19035|\n",
      "|2021-02-05 11:02:24|2021-02-05 16:17:28|   18904|\n",
      "|2021-02-16 17:02:02|2021-02-16 22:13:23|   18681|\n",
      "|2021-02-06 21:33:47|2021-02-07 02:40:16|   18389|\n",
      "|2021-02-12 14:59:22|2021-02-12 19:50:20|   17458|\n",
      "|2021-02-19 22:44:04|2021-02-20 03:34:23|   17419|\n",
      "+-------------------+-------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pickup_datetime, dropoff_datetime,\n",
    "    (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) AS duration\n",
    "FROM\n",
    "    fhvhv\n",
    "SORT BY\n",
    "    duration DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "\n",
    "# method 1\n",
    "df \\\n",
    "    .withColumn('duration', df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long')) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()\n",
    "\n",
    "# method 2\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60) AS duration\n",
    "FROM \n",
    "    fhvhv_2021_02\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 answer (correct)\n",
    "\n",
    "```\n",
    "February 11th\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. Most frequent `dispatching_base_num`\n",
    "\n",
    "Now find the most frequently occurring `dispatching_base_num` \n",
    "in this dataset.\n",
    "\n",
    "How many stages this spark job has?\n",
    "\n",
    "> Note: the answer may depend on how you write the query,\n",
    "> so there are multiple correct answers. \n",
    "> Select the one you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==================================>                    (124 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+\n",
      "|dispatching_base_num|count(dispatching_base_num)|\n",
      "+--------------------+---------------------------+\n",
      "|              B02510|                    3233664|\n",
      "|              B02764|                     965568|\n",
      "|              B02872|                     882689|\n",
      "|              B02875|                     685390|\n",
      "|              B02765|                     559768|\n",
      "|              B02869|                     429720|\n",
      "|              B02887|                     322331|\n",
      "|              B02871|                     312364|\n",
      "|              B02864|                     311603|\n",
      "|              B02866|                     311089|\n",
      "|              B02878|                     305185|\n",
      "|              B02682|                     303255|\n",
      "|              B02617|                     274510|\n",
      "|              B02883|                     251617|\n",
      "|              B02884|                     244963|\n",
      "|              B02882|                     232173|\n",
      "|              B02876|                     215693|\n",
      "|              B02879|                     210137|\n",
      "|              B02867|                     200530|\n",
      "|              B02877|                     198938|\n",
      "+--------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    dispatching_base_num,\n",
    "    COUNT(dispatching_base_num)\n",
    "FROM\n",
    "    fhvhv\n",
    "GROUP BY\n",
    "    dispatching_base_num\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    dispatching_base_num,\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhvhv_2021_02\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()\n",
    "\n",
    "df \\\n",
    "    .groupBy('dispatching_base_num') \\\n",
    "        .count() \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 answer (correct)\n",
    "\n",
    "```\n",
    "2 stages\n",
    "```\n",
    "\n",
    "Note that if you use `LIMIT 5` or any other number, an additional stage will be added.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. Most common locations pair\n",
    "\n",
    "Find the most common pickup-dropoff pair. \n",
    "\n",
    "For example:\n",
    "\n",
    "\"Jamaica Bay / Clinton East\"\n",
    "\n",
    "Enter two zone names separated by a slash\n",
    "\n",
    "If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "zpu = df_zones \\\n",
    "    .withColumnRenamed('Zone', 'PUzone') \\\n",
    "    .withColumnRenamed('LocationID', 'zPULocationID') \\\n",
    "    .withColumnRenamed('Borough', 'PUBorough') \\\n",
    "    .drop('service_zone')\n",
    "zdo = df_zones \\\n",
    "    .withColumnRenamed('Zone', 'DOzone') \\\n",
    "    .withColumnRenamed('LocationID', 'zDOLocationID') \\\n",
    "    .withColumnRenamed('Borough', 'DOBorough') \\\n",
    "    .drop('service_zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_temp = df.join(zpu, df.PULocationID == zpu.zPULocationID)\n",
    "df_join = df_join_temp.join(zdo, df_join_temp.DOLocationID == zdo.zDOLocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.drop('PULocationID', 'DOLocationID', 'zPULocationID', 'zDOLocationID').write.parquet('tmp/homework/6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, SR_Flag: string, PUBorough: string, PUzone: string, DOBorough: string, DOzone: string]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join = spark.read.parquet('tmp/homework/6')\n",
    "df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.registerTempTable('join_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:===================================================>  (191 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|           zone_pair|count(1)|\n",
      "+--------------------+--------+\n",
      "|East New York/Eas...|   45041|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    CONCAT(coalesce(PUzone, 'Unknown'), '/', coalesce(DOzone, 'Unknown')) AS zone_pair,\n",
    "    COUNT(1)\n",
    "FROM\n",
    "    join_table\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT\n",
    "    1\n",
    ";\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones')\n",
    "\n",
    "df_zones.columns\n",
    "\n",
    "df.columns\n",
    "\n",
    "df_zones.registerTempTable('zones')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    CONCAT(pul.Zone, ' / ', dol.Zone) AS pu_do_pair,\n",
    "    COUNT(1)\n",
    "FROM \n",
    "    fhvhv_2021_02 fhv LEFT JOIN zones pul ON fhv.PULocationID = pul.LocationID\n",
    "                      LEFT JOIN zones dol ON fhv.DOLocationID = dol.LocationID\n",
    "GROUP BY \n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 answer (correct)\n",
    "\n",
    "```\n",
    "East New York / East New York\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus question. Join type\n",
    "\n",
    "(not graded) \n",
    "\n",
    "For finding the answer to Q6, you'll need to perform a join.\n",
    "\n",
    "What type of join is it?\n",
    "\n",
    "And how many stages your spark job has?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus question answer\n",
    "\n",
    "```\n",
    "inner join\n",
    "\n",
    "The SQL query has 3 stages.\n",
    "```\n",
    "\n",
    "### Actual answer\n",
    "\n",
    "Broadcast join, because we broadcast the zones to the executors and join on the fly."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cc142a295aadb3a4a4b07e4cf623a190b887fd9d0099585537187a2dbcd9102"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
